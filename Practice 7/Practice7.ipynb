{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRsSn7f4BTRg"
   },
   "source": [
    "# 7: Practice\n",
    "In this exercise, you will practice using deep learning with artificial neural networks and the Pytorch library to build classification models. Note that Pytorch is **not** included in the Anaconda distribution by default. You have three options for using Pytorch to complete this assignment:\n",
    "\n",
    "1. Install Pytorch locally (for free). You can see the directions on the [Pytorch website here](https://pytorch.org/get-started/locally/). Select the stable build, your operating system, Conda (for Anaconda), Python, and CPU to see install directions for your particular setup. (CUDA is used to support hardware acceleration with NVIDIA graphics cards and is not necessary for this course).\n",
    "\n",
    "1. Use a [Duke OIT container](https://cmgr.oit.duke.edu/containers). Make sure to use the **PyTorch** one. If you have not already reserved your container, see the class website's [Resources & Getting Help](https://sites.duke.edu/compsci216f2021/resources-getting-help/) page.\n",
    "\n",
    "1. Use Pytorch in a Jupyter notebook in the cloud (also for free). The easiest way to do this is with a Google colab notebook; Pytorch will already be available to you in this cloud environment. If you choose this option, go to the [Google colab website](https://colab.research.google.com) select \"Upload\" at the right side of the orange menu bar, and select this `.ipynb` file. You can then complete the assignment in colab and download the completed `.ipynb` file to turn in on Gradescope.\n",
    "\n",
    "This practice assignment is tutorial style; in general we will provide the necessary code and simply ask you to run the code, observe what happens, and answer questions based on the code. One of the questions you will answer as a Sakai Quiz (with unlimited attempts), the rest you will answer in this notebook. The code in this assignment is based on the image classification example presented in the [Deep Learning with Pytorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) tutorial and the [Pytorch Basics Tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html#). You are welcome and encouraged to visit these and other Pytorch [tutorials](https://pytorch.org/tutorials/), [documentation](https://pytorch.org/docs/stable/index.html), and [recipes/examples](https://pytorch.org/tutorials/recipes/recipes_index.html) as needed or for your own interest and learning.\n",
    "\n",
    "When you finish please confirm that your notebook and its output looks correct before submitting your .ipynb file (the notebook file) on gradescope. If you select Kernel -> Restart and Run All, please allow time for the code to finish executing; the code in this notebook may take a few minutes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAAajdJaBTRs"
   },
   "source": [
    "### Question 1: The Data\n",
    "First, we import the necessary libraries and download the `CIFAR-10` dataset from Pytorch. The `CIFAR-10` dataset consists of 60,000 32 by 32 pixel color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. The classes of the images are things like `airplane`, `automobile`, `bird`, `cat`, `deer`, etc. You can read more about the dataset at the [curator's website](https://www.cs.toronto.edu/~kriz/cifar.html). \n",
    "\n",
    "Run the following code to import necessary libraries, download the data, and preview some of the images along with their classes. **Note that this step may take a few minutes depending on your internet speed**; the dataset is about 160 MB. Once you have downloaded the data once, rerunning the cell should be fast; it will verify that you already have the dataset rather than redownload every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "zotjaYfLCVcw"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1\n",
    "manual: true\n",
    "points:\n",
    "    - 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "t1KoIi3IBTRw"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-f0415ac78a23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Imports all of the necessary Pytorch libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Run but do not modify this code\n",
    "# Imports all of the necessary Pytorch libraries\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "2aaf32ce2de2425c9ad0c662592fd36c",
      "60dc78afcde44cf2b7f4134b0d264693",
      "fbaf8a68cbf741b3a184649962aef2c0",
      "533576a7f6074b2f8a098b69eaa03365",
      "8145b42aa9c647b9a9a3af614a027fc7",
      "75c5113decb34b4fbc0e4388138e7fd6",
      "0dcd387494334475bd6eed273d52fc73",
      "fdb3ae48121c4ec1b910d2c0033d197e",
      "8cccac49e7714195810e5bb18ba297d1",
      "19e42179a82d4250b302238f0b28bbe7",
      "acc13af80b874972bd10f4055e6823fc"
     ]
    },
    "id": "s-_Ku1iHBTR5",
    "outputId": "0aedeb3c-2109-4e2b-f6fe-aa67508b9f3d"
   },
   "outputs": [],
   "source": [
    "# Run but do not modify this code\n",
    "# Downloads, imports, and formats the CIFAR-10 image dataset. \n",
    "# May take a few minutes the first time you run it\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_dataloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "id": "fVK2nCq4BTR9",
    "outputId": "eb00d694-3ce6-4eee-a342-882735c2a38f"
   },
   "outputs": [],
   "source": [
    "# Run but do not modify this code\n",
    "# Displays random images from the CIFAR-10 image dataset\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_dataloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKOe7EaaBTR_"
   },
   "source": [
    "After running this code, `images` is a tensor containing the images displayed above. Run the code below to see the shape of the tensor, which is 4 by 3 by 32 by 32. Explain these values: What do these numbers represent in terms of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6eTgzrG-BTSA",
    "outputId": "12d21f01-020a-490b-a6ec-d9f0d4e969cf"
   },
   "outputs": [],
   "source": [
    "# Run but do not modify this code\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiPiK2__CBK3",
    "tags": []
   },
   "source": [
    "**Please write your answer for question 1 here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Answer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The first number is the batch size. The second number means 3 matrices from each image. And the first 32 is the height and the second 32 is the width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ch3WK3XfBTSJ"
   },
   "source": [
    "### Question 2\n",
    "Now we will start by defining a (relatively) small multilayer perceptron for classifying these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jAk-IijBTSK"
   },
   "outputs": [],
   "source": [
    "# Run but do not modify this code\n",
    "# Defines a multilayer perceptron class\n",
    "\n",
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(32*32*3, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPC0O8CnBTSO"
   },
   "source": [
    "Answer the following questions about this `MultilayerPerceptron` defined in the code above.\n",
    "1. How many neurons are there in the input layer of the neural network?\n",
    "2. How many **hidden** (that is, apart from the input and output layers) fully connected linear layers are there in the network?\n",
    "3. What nonlinear activation function is the network using?\n",
    "4. How many parameters are there for the connection between the input layer and the first hidden fully connected layer in the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjALtYqXqNRk"
   },
   "source": [
    "**Please fill your answers to question2 on Sakai**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wdy1o4jvBTSS"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "### Question 3\n",
    "Now we define our hyperparameters, initialize our model and optimizer, and define our training and testing process. The training uses stochastic gradient descent and prints the average training loss after every 1,250 batches or 5,000 images. After each of three epochs of training we print the average accuracy of the model on the held out test data; this is the percentage of the test images that our model correctly classifies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7Cf39J9BTSS"
   },
   "outputs": [],
   "source": [
    "# Run but do not modify this code\n",
    "# Defines hyperparameters \n",
    "\n",
    "torch.manual_seed(216)\n",
    "learning_rate = 1e-3\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjRrw7ZABTST"
   },
   "outputs": [],
   "source": [
    "# Run but do not modify this code\n",
    "# Initializes the network, loss function, and optimizer\n",
    "\n",
    "model = MultilayerPerceptron()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05RKXAS2BTSV"
   },
   "outputs": [],
   "source": [
    "# Run but do not modify this code\n",
    "# Defines the training optimization process and\n",
    "# calculating accuracy on the held out testing data\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    avg_batch_loss = 0.0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_batch_loss += loss.item()\n",
    "\n",
    "        # Print average loss every 1250 batches\n",
    "        if ((batch > 0) and (batch % 1250 == 0)):\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"Average Loss: {avg_batch_loss/1250:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            avg_batch_loss = 0.0\n",
    "            \n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIBCV0I1BTSZ",
    "outputId": "47cc2ce6-3023-4f2e-c171-5658d57cd1b5"
   },
   "outputs": [],
   "source": [
    "# Run but do not modify this code\n",
    "# Performs 'epochs' of training with stochastic\n",
    "# gradient descent, printing average loss of every\n",
    "# 1250 batches. Prints test accuracy after each\n",
    "# epoch.\n",
    "# NOTE: Optimization on 50,000 training samples may take\n",
    "# some time, expect this code to run for several seconds \n",
    "# to a couple of minutes.\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7vfsEgiBTSb"
   },
   "source": [
    "Answer the following questions about the training/testing in the code above. For parts 2, 3, and 4 answer the first question in Sakai and include that answer in your response to the second question to make manual grading easier.\n",
    "\n",
    "1. There are 10 neurons in the output layer of the model. How does the code in `test_loop` predict a single class as an integer given the 10 values output by the model during forward propagation?\n",
    "\n",
    "2. After 3 epochs of training, is our model performing better than randomly guessing the class? If yes, how much? If no, how can you tell?\n",
    "\n",
    "3. Suppose we double the sizes of the hidden fully connected layers in the model. Would you expect the model to achieve better (lower) or worse (higher) loss during training? Briefly explain your answer.\n",
    "\n",
    "4. Again suppose we double the sizes of the hidden fully connected layers in the model. Would you expect training the model with stochastic gradient descent over two epochs to take less or more time? Briefly explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please also answer the multiple choice questions on Sakai to test your understanding and get full credit for question3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3\n",
    "1. It predicts a single class by condensing the data by taking an average of the overall values which gives a different value. \n",
    "2. Yes, after 3 epochs the model is performing better because the model has an accuracy of 35% which is higher than the expected 10%.\n",
    "3. Better, because making size bigger should increase the overall accuracy. \n",
    "4. More, time because doubling the size is more layers which means more computations so the runtime increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DyaLielBTSb"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "### Question 4\n",
    "Now we define a convolutional neural network for the same classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8n8zOZ3WBTSc"
   },
   "outputs": [],
   "source": [
    "# Run but do not modify this code\n",
    "# Defines a convolutional neural network class\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv_pool_stack = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(16*5*5, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        convolved = self.conv_pool_stack(x)\n",
    "        flattened = self.flatten(convolved)\n",
    "        logits = self.linear_relu_stack(flattened)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpjlqDSdBTSc"
   },
   "source": [
    "Answer the following questions about the `ConvNet` class defined in the code above. For each part, answer the question in Sakai and then include your answer with a brief explanation of your answer in the notebook. You do not need to give the exact number of parameters.\n",
    "\n",
    "1. The first hidden fully connected linear layer in the network, after convolving, pooling, and flattening, has $16 \\times 5 \\times 5 = 400$ neurons. This is much less than the size of the input for the `MultilayerPerceptron` model. What part of the `ConvNet` most accounts for this reduction in the input size?\n",
    "\n",
    "2. Are there more parameters in the `conv_pool_stack` or the `linear_relu_stack` portions of the network?\n",
    "\n",
    "3. Does this model have more or fewer total parameters than the `MultilayerPerceptron` defined previously?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please also answer the multiple choice questions on Sakai to test your understanding and get full credit for question4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Answer4\n",
    "1. The part of the ConvNet that most accounts for this is the pooling because each matrix only has one value which makes the size of the input smaller.\n",
    "\n",
    "2. More in the linear_relu_stack because there are more neurons which means more to deal with. \n",
    "\n",
    "3. The model has less parameters because the overall inputs are lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9Yq31hwBTSd"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "### Question 5\n",
    "Now we define our hyperparameters, initialize our model and optimizer, and define our training and testing process. As for the `MultilayerPerceptron`, the training uses stochastic gradient descent and prints the average training loss after every 1,250 batches or 5,000 images. After each of three epochs of training we print the average accuracy of the model on the held out test data; this is the percentage of the test images that our model correctly classifies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "tSSczWH_r9TZ"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5\n",
    "manual: true\n",
    "points:\n",
    "    - 4\n",
    "    - 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqpZ17HxBTSd"
   },
   "outputs": [],
   "source": [
    "# Run but do not modify this code\n",
    "\n",
    "torch.manual_seed(216)\n",
    "learning_rate = 1e-3\n",
    "epochs = 3\n",
    "conv_model = ConvNet()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(conv_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NntqWnuABTSi",
    "outputId": "90c15836-e6e1-41ed-f02a-6b8c501b2aca"
   },
   "outputs": [],
   "source": [
    "# Run but do not modify this code\n",
    "# Performs 'epochs' of training with stochastic\n",
    "# gradient descent, printing average loss of every\n",
    "# 1250 batches. Prints test accuracy after each\n",
    "# epoch.\n",
    "# NOTE: Optimization on 50,000 training samples may take\n",
    "# some time, expect this code to run for several seconds \n",
    "# to a couple of minutes.\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, conv_model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, conv_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ii8vS7jeBTSj"
   },
   "source": [
    "Answer the following questions about this the training/testing in the code above. For part 2 answer the first question in Sakai and include that answer in your explanation to make manual grading easier.\n",
    "\n",
    "1. Compare the results with the `ConvNet` to those obtained with the `MultilayerPerceptron`. In particular, what difference do you observe about the training loss of the two models during the third epoch of training?\n",
    "\n",
    "2. Based on your observation of the training loss of the `ConvNet` during the third epoch of training, do you think increasing the `learning_rate` (the step size of stochastic gradient descent) somewhat (say, from 0.001 to 0.01) would result in better or worse accuracy during testing after three epochs of training? Briefly explain your answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Answer5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please also answer the multiple choice questions on Sakai to test your understanding and get full credit for question5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. There are multiple differences between these two models. As far as accuracy, the ConvNet had more overall improvement than the MultilayerPerceptron. The overall ConvNet increased from 23% to 40% while the MultilayerPerceptron increased from 27% to 35%. Furthermore, the MultiLayer had worse improvement with the overall loss. ConvNet went from a higher average loss to a lower average loss over time. This tells us that the Convnet became better after 3 epochs. So the ConvNet performs better overall.\n",
    "\n",
    "2. The learning rat has a definite impact and so I would increase the step size for better accuracy because the network weights are better affected and more accurate. The 0.001 is less than the 0.01 of the weight error. Furthermore, due to back propogation, the higher 0.01 will update more making the model more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
